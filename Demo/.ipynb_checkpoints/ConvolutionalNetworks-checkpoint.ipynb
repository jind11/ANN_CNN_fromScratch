{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convolutional Networks\n",
    "This is the demo for convolution neural network which is used to classify CIFAR10 dataset here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from Model.convnet import *\n",
    "from Dataset.data_utils import get_CIFAR10_data\n",
    "from Solver.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000, 3, 32, 32)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (49000,)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 12250) loss: 2.308168\n",
      "(Epoch 0 / 50) train acc: 0.114000; val_acc: 0.118000\n",
      "(Iteration 21 / 12250) loss: 2.268943\n",
      "(Iteration 41 / 12250) loss: 2.249681\n",
      "(Iteration 61 / 12250) loss: 2.237493\n",
      "(Iteration 81 / 12250) loss: 2.215893\n",
      "(Iteration 101 / 12250) loss: 2.208433\n",
      "(Iteration 121 / 12250) loss: 2.180398\n",
      "(Iteration 141 / 12250) loss: 2.155566\n",
      "(Iteration 161 / 12250) loss: 2.143117\n",
      "(Iteration 181 / 12250) loss: 2.120898\n",
      "(Iteration 201 / 12250) loss: 2.118120\n",
      "(Iteration 221 / 12250) loss: 2.102424\n",
      "(Iteration 241 / 12250) loss: 2.054974\n",
      "(Epoch 1 / 50) train acc: 0.492000; val_acc: 0.516000\n",
      "(Iteration 261 / 12250) loss: 2.064706\n",
      "(Iteration 281 / 12250) loss: 2.042702\n",
      "(Iteration 301 / 12250) loss: 2.038244\n",
      "(Iteration 321 / 12250) loss: 2.003797\n",
      "(Iteration 341 / 12250) loss: 1.977176\n",
      "(Iteration 361 / 12250) loss: 1.983368\n",
      "(Iteration 381 / 12250) loss: 1.936832\n",
      "(Iteration 401 / 12250) loss: 1.973487\n",
      "(Iteration 421 / 12250) loss: 1.950850\n",
      "(Iteration 441 / 12250) loss: 1.947986\n",
      "(Iteration 461 / 12250) loss: 1.898426\n",
      "(Iteration 481 / 12250) loss: 1.914319\n",
      "(Epoch 2 / 50) train acc: 0.558000; val_acc: 0.566000\n",
      "(Iteration 501 / 12250) loss: 1.905606\n",
      "(Iteration 521 / 12250) loss: 1.875547\n",
      "(Iteration 541 / 12250) loss: 1.855876\n",
      "(Iteration 561 / 12250) loss: 1.847851\n",
      "(Iteration 581 / 12250) loss: 1.858566\n",
      "(Iteration 601 / 12250) loss: 1.836335\n",
      "(Iteration 621 / 12250) loss: 1.831487\n",
      "(Iteration 641 / 12250) loss: 1.830266\n",
      "(Iteration 661 / 12250) loss: 1.813640\n",
      "(Iteration 681 / 12250) loss: 1.799101\n",
      "(Iteration 701 / 12250) loss: 1.746720\n",
      "(Iteration 721 / 12250) loss: 1.728497\n",
      "(Epoch 3 / 50) train acc: 0.588000; val_acc: 0.588000\n",
      "(Iteration 741 / 12250) loss: 1.759279\n",
      "(Iteration 761 / 12250) loss: 1.759772\n",
      "(Iteration 781 / 12250) loss: 1.711298\n",
      "(Iteration 801 / 12250) loss: 1.754406\n",
      "(Iteration 821 / 12250) loss: 1.704803\n",
      "(Iteration 841 / 12250) loss: 1.719878\n",
      "(Iteration 861 / 12250) loss: 1.681526\n",
      "(Iteration 881 / 12250) loss: 1.647738\n",
      "(Iteration 901 / 12250) loss: 1.681511\n",
      "(Iteration 921 / 12250) loss: 1.674838\n",
      "(Iteration 941 / 12250) loss: 1.633669\n",
      "(Iteration 961 / 12250) loss: 1.681138\n",
      "(Epoch 4 / 50) train acc: 0.613000; val_acc: 0.595000\n",
      "(Iteration 981 / 12250) loss: 1.662079\n",
      "(Iteration 1001 / 12250) loss: 1.612743\n",
      "(Iteration 1021 / 12250) loss: 1.585693\n",
      "(Iteration 1041 / 12250) loss: 1.649750\n",
      "(Iteration 1061 / 12250) loss: 1.607746\n",
      "(Iteration 1081 / 12250) loss: 1.616904\n",
      "(Iteration 1101 / 12250) loss: 1.623308\n",
      "(Iteration 1121 / 12250) loss: 1.611608\n",
      "(Iteration 1141 / 12250) loss: 1.606442\n",
      "(Iteration 1161 / 12250) loss: 1.561056\n",
      "(Iteration 1181 / 12250) loss: 1.595972\n",
      "(Iteration 1201 / 12250) loss: 1.515930\n",
      "(Iteration 1221 / 12250) loss: 1.525293\n",
      "(Epoch 5 / 50) train acc: 0.636000; val_acc: 0.628000\n",
      "(Iteration 1241 / 12250) loss: 1.488044\n",
      "(Iteration 1261 / 12250) loss: 1.518913\n",
      "(Iteration 1281 / 12250) loss: 1.535192\n",
      "(Iteration 1301 / 12250) loss: 1.533564\n",
      "(Iteration 1321 / 12250) loss: 1.490253\n",
      "(Iteration 1341 / 12250) loss: 1.509479\n",
      "(Iteration 1361 / 12250) loss: 1.500795\n",
      "(Iteration 1381 / 12250) loss: 1.567655\n",
      "(Iteration 1401 / 12250) loss: 1.446120\n",
      "(Iteration 1421 / 12250) loss: 1.513362\n",
      "(Iteration 1441 / 12250) loss: 1.432234\n",
      "(Iteration 1461 / 12250) loss: 1.500901\n",
      "(Epoch 6 / 50) train acc: 0.658000; val_acc: 0.643000\n",
      "(Iteration 1481 / 12250) loss: 1.440008\n",
      "(Iteration 1501 / 12250) loss: 1.456617\n",
      "(Iteration 1521 / 12250) loss: 1.437335\n",
      "(Iteration 1541 / 12250) loss: 1.398265\n",
      "(Iteration 1561 / 12250) loss: 1.432115\n",
      "(Iteration 1581 / 12250) loss: 1.432749\n",
      "(Iteration 1601 / 12250) loss: 1.418445\n",
      "(Iteration 1621 / 12250) loss: 1.364166\n",
      "(Iteration 1641 / 12250) loss: 1.381185\n",
      "(Iteration 1661 / 12250) loss: 1.394050\n",
      "(Iteration 1681 / 12250) loss: 1.354544\n",
      "(Iteration 1701 / 12250) loss: 1.373755\n",
      "(Epoch 7 / 50) train acc: 0.666000; val_acc: 0.645000\n",
      "(Iteration 1721 / 12250) loss: 1.345783\n",
      "(Iteration 1741 / 12250) loss: 1.439953\n",
      "(Iteration 1761 / 12250) loss: 1.354738\n",
      "(Iteration 1781 / 12250) loss: 1.353529\n",
      "(Iteration 1801 / 12250) loss: 1.353060\n",
      "(Iteration 1821 / 12250) loss: 1.386741\n",
      "(Iteration 1841 / 12250) loss: 1.383632\n",
      "(Iteration 1861 / 12250) loss: 1.320971\n",
      "(Iteration 1881 / 12250) loss: 1.346408\n",
      "(Iteration 1901 / 12250) loss: 1.357915\n",
      "(Iteration 1921 / 12250) loss: 1.343319\n",
      "(Iteration 1941 / 12250) loss: 1.352403\n",
      "(Epoch 8 / 50) train acc: 0.677000; val_acc: 0.654000\n",
      "(Iteration 1961 / 12250) loss: 1.346610\n",
      "(Iteration 1981 / 12250) loss: 1.300442\n",
      "(Iteration 2001 / 12250) loss: 1.312932\n",
      "(Iteration 2021 / 12250) loss: 1.285073\n",
      "(Iteration 2041 / 12250) loss: 1.274729\n",
      "(Iteration 2061 / 12250) loss: 1.313049\n",
      "(Iteration 2081 / 12250) loss: 1.294634\n",
      "(Iteration 2101 / 12250) loss: 1.291566\n",
      "(Iteration 2121 / 12250) loss: 1.252211\n",
      "(Iteration 2141 / 12250) loss: 1.292430\n",
      "(Iteration 2161 / 12250) loss: 1.276369\n",
      "(Iteration 2181 / 12250) loss: 1.322467\n",
      "(Iteration 2201 / 12250) loss: 1.291817\n",
      "(Epoch 9 / 50) train acc: 0.711000; val_acc: 0.675000\n",
      "(Iteration 2221 / 12250) loss: 1.265103\n",
      "(Iteration 2241 / 12250) loss: 1.253871\n",
      "(Iteration 2261 / 12250) loss: 1.226598\n",
      "(Iteration 2281 / 12250) loss: 1.203248\n",
      "(Iteration 2301 / 12250) loss: 1.268107\n",
      "(Iteration 2321 / 12250) loss: 1.229774\n",
      "(Iteration 2341 / 12250) loss: 1.174885\n",
      "(Iteration 2361 / 12250) loss: 1.256013\n",
      "(Iteration 2381 / 12250) loss: 1.240042\n",
      "(Iteration 2401 / 12250) loss: 1.230591\n",
      "(Iteration 2421 / 12250) loss: 1.203554\n",
      "(Iteration 2441 / 12250) loss: 1.191509\n",
      "(Epoch 10 / 50) train acc: 0.713000; val_acc: 0.686000\n",
      "(Iteration 2461 / 12250) loss: 1.151600\n",
      "(Iteration 2481 / 12250) loss: 1.220351\n",
      "(Iteration 2501 / 12250) loss: 1.207832\n",
      "(Iteration 2521 / 12250) loss: 1.253514\n",
      "(Iteration 2541 / 12250) loss: 1.217443\n",
      "(Iteration 2561 / 12250) loss: 1.156096\n",
      "(Iteration 2581 / 12250) loss: 1.142857\n",
      "(Iteration 2601 / 12250) loss: 1.190745\n",
      "(Iteration 2621 / 12250) loss: 1.183139\n",
      "(Iteration 2641 / 12250) loss: 1.140728\n",
      "(Iteration 2661 / 12250) loss: 1.108905\n",
      "(Iteration 2681 / 12250) loss: 1.175788\n",
      "(Epoch 11 / 50) train acc: 0.735000; val_acc: 0.692000\n",
      "(Iteration 2701 / 12250) loss: 1.129952\n",
      "(Iteration 2721 / 12250) loss: 1.135469\n",
      "(Iteration 2741 / 12250) loss: 1.134916\n",
      "(Iteration 2761 / 12250) loss: 1.150251\n",
      "(Iteration 2781 / 12250) loss: 1.160153\n",
      "(Iteration 2801 / 12250) loss: 1.135668\n",
      "(Iteration 2821 / 12250) loss: 1.140915\n",
      "(Iteration 2841 / 12250) loss: 1.084269\n",
      "(Iteration 2861 / 12250) loss: 1.202188\n",
      "(Iteration 2881 / 12250) loss: 1.143533\n",
      "(Iteration 2901 / 12250) loss: 1.192839\n",
      "(Iteration 2921 / 12250) loss: 1.167415\n",
      "(Epoch 12 / 50) train acc: 0.744000; val_acc: 0.702000\n",
      "(Iteration 2941 / 12250) loss: 1.132659\n",
      "(Iteration 2961 / 12250) loss: 1.141176\n",
      "(Iteration 2981 / 12250) loss: 1.070887\n",
      "(Iteration 3001 / 12250) loss: 1.093384\n",
      "(Iteration 3021 / 12250) loss: 1.087543\n",
      "(Iteration 3041 / 12250) loss: 1.115053\n",
      "(Iteration 3061 / 12250) loss: 1.092794\n",
      "(Iteration 3081 / 12250) loss: 1.168767\n",
      "(Iteration 3101 / 12250) loss: 1.073154\n",
      "(Iteration 3121 / 12250) loss: 1.098684\n",
      "(Iteration 3141 / 12250) loss: 1.031803\n",
      "(Iteration 3161 / 12250) loss: 1.064468\n",
      "(Iteration 3181 / 12250) loss: 1.078864\n",
      "(Epoch 13 / 50) train acc: 0.732000; val_acc: 0.702000\n",
      "(Iteration 3201 / 12250) loss: 1.046330\n",
      "(Iteration 3221 / 12250) loss: 1.081094\n",
      "(Iteration 3241 / 12250) loss: 1.094183\n",
      "(Iteration 3261 / 12250) loss: 1.109788\n",
      "(Iteration 3281 / 12250) loss: 1.049539\n",
      "(Iteration 3301 / 12250) loss: 1.122131\n",
      "(Iteration 3321 / 12250) loss: 1.105548\n",
      "(Iteration 3341 / 12250) loss: 1.023724\n",
      "(Iteration 3361 / 12250) loss: 1.093350\n",
      "(Iteration 3381 / 12250) loss: 1.076707\n",
      "(Iteration 3401 / 12250) loss: 1.066423\n",
      "(Iteration 3421 / 12250) loss: 1.115152\n",
      "(Epoch 14 / 50) train acc: 0.736000; val_acc: 0.701000\n",
      "(Iteration 3441 / 12250) loss: 1.097591\n",
      "(Iteration 3461 / 12250) loss: 1.051009\n",
      "(Iteration 3481 / 12250) loss: 1.100897\n",
      "(Iteration 3501 / 12250) loss: 1.070395\n",
      "(Iteration 3521 / 12250) loss: 1.073327\n",
      "(Iteration 3541 / 12250) loss: 1.059279\n",
      "(Iteration 3561 / 12250) loss: 1.020763\n",
      "(Iteration 3581 / 12250) loss: 1.060872\n",
      "(Iteration 3601 / 12250) loss: 1.118825\n",
      "(Iteration 3621 / 12250) loss: 1.091189\n",
      "(Iteration 3641 / 12250) loss: 1.047748\n",
      "(Iteration 3661 / 12250) loss: 1.046850\n",
      "(Epoch 15 / 50) train acc: 0.774000; val_acc: 0.713000\n",
      "(Iteration 3681 / 12250) loss: 0.991348\n",
      "(Iteration 3701 / 12250) loss: 0.971994\n",
      "(Iteration 3721 / 12250) loss: 1.075862\n",
      "(Iteration 3741 / 12250) loss: 1.001077\n",
      "(Iteration 3761 / 12250) loss: 1.092620\n",
      "(Iteration 3781 / 12250) loss: 0.991560\n",
      "(Iteration 3801 / 12250) loss: 1.078365\n",
      "(Iteration 3821 / 12250) loss: 1.133824\n",
      "(Iteration 3841 / 12250) loss: 0.983706\n",
      "(Iteration 3861 / 12250) loss: 0.940766\n",
      "(Iteration 3881 / 12250) loss: 1.031417\n",
      "(Iteration 3901 / 12250) loss: 0.953708\n",
      "(Epoch 16 / 50) train acc: 0.736000; val_acc: 0.713000\n",
      "(Iteration 3921 / 12250) loss: 0.986977\n",
      "(Iteration 3941 / 12250) loss: 1.151841\n",
      "(Iteration 3961 / 12250) loss: 1.054837\n",
      "(Iteration 3981 / 12250) loss: 0.965446\n",
      "(Iteration 4001 / 12250) loss: 0.999746\n",
      "(Iteration 4021 / 12250) loss: 0.964971\n",
      "(Iteration 4041 / 12250) loss: 1.080992\n",
      "(Iteration 4061 / 12250) loss: 1.034810\n",
      "(Iteration 4081 / 12250) loss: 0.961745\n",
      "(Iteration 4101 / 12250) loss: 1.041194\n",
      "(Iteration 4121 / 12250) loss: 0.940358\n",
      "(Iteration 4141 / 12250) loss: 1.046527\n",
      "(Iteration 4161 / 12250) loss: 1.125180\n",
      "(Epoch 17 / 50) train acc: 0.740000; val_acc: 0.717000\n",
      "(Iteration 4181 / 12250) loss: 0.928527\n",
      "(Iteration 4201 / 12250) loss: 1.020672\n",
      "(Iteration 4221 / 12250) loss: 1.014379\n",
      "(Iteration 4241 / 12250) loss: 0.983303\n",
      "(Iteration 4261 / 12250) loss: 0.998616\n",
      "(Iteration 4281 / 12250) loss: 0.960228\n",
      "(Iteration 4301 / 12250) loss: 0.877739\n",
      "(Iteration 4321 / 12250) loss: 0.944079\n",
      "(Iteration 4341 / 12250) loss: 0.871246\n",
      "(Iteration 4361 / 12250) loss: 0.975408\n",
      "(Iteration 4381 / 12250) loss: 0.927344\n",
      "(Iteration 4401 / 12250) loss: 0.959422\n",
      "(Epoch 18 / 50) train acc: 0.766000; val_acc: 0.720000\n",
      "(Iteration 4421 / 12250) loss: 0.959499\n",
      "(Iteration 4441 / 12250) loss: 0.904675\n",
      "(Iteration 4461 / 12250) loss: 0.999189\n",
      "(Iteration 4481 / 12250) loss: 0.969587\n",
      "(Iteration 4501 / 12250) loss: 0.907018\n",
      "(Iteration 4521 / 12250) loss: 0.880927\n",
      "(Iteration 4541 / 12250) loss: 0.925950\n",
      "(Iteration 4561 / 12250) loss: 0.988912\n",
      "(Iteration 4581 / 12250) loss: 0.973706\n",
      "(Iteration 4601 / 12250) loss: 0.968218\n",
      "(Iteration 4621 / 12250) loss: 1.017444\n",
      "(Iteration 4641 / 12250) loss: 1.001638\n",
      "(Epoch 19 / 50) train acc: 0.766000; val_acc: 0.717000\n",
      "(Iteration 4661 / 12250) loss: 0.927653\n",
      "(Iteration 4681 / 12250) loss: 0.891656\n",
      "(Iteration 4701 / 12250) loss: 0.909398\n",
      "(Iteration 4721 / 12250) loss: 0.910066\n",
      "(Iteration 4741 / 12250) loss: 0.915295\n",
      "(Iteration 4761 / 12250) loss: 0.882786\n",
      "(Iteration 4781 / 12250) loss: 0.862469\n",
      "(Iteration 4801 / 12250) loss: 0.984080\n",
      "(Iteration 4821 / 12250) loss: 0.934593\n",
      "(Iteration 4841 / 12250) loss: 0.884747\n",
      "(Iteration 4861 / 12250) loss: 0.968119\n",
      "(Iteration 4881 / 12250) loss: 0.932276\n",
      "(Epoch 20 / 50) train acc: 0.799000; val_acc: 0.729000\n",
      "(Iteration 4901 / 12250) loss: 0.885166\n",
      "(Iteration 4921 / 12250) loss: 0.877818\n",
      "(Iteration 4941 / 12250) loss: 0.974811\n",
      "(Iteration 4961 / 12250) loss: 0.882123\n",
      "(Iteration 4981 / 12250) loss: 0.891321\n",
      "(Iteration 5001 / 12250) loss: 0.887699\n",
      "(Iteration 5021 / 12250) loss: 0.897497\n",
      "(Iteration 5041 / 12250) loss: 0.892611\n",
      "(Iteration 5061 / 12250) loss: 0.901084\n",
      "(Iteration 5081 / 12250) loss: 1.017785\n",
      "(Iteration 5101 / 12250) loss: 0.870115\n",
      "(Iteration 5121 / 12250) loss: 0.984932\n",
      "(Iteration 5141 / 12250) loss: 0.854480\n",
      "(Epoch 21 / 50) train acc: 0.776000; val_acc: 0.725000\n",
      "(Iteration 5161 / 12250) loss: 0.915552\n",
      "(Iteration 5181 / 12250) loss: 0.952263\n",
      "(Iteration 5201 / 12250) loss: 0.901074\n",
      "(Iteration 5221 / 12250) loss: 0.873461\n",
      "(Iteration 5241 / 12250) loss: 0.923003\n",
      "(Iteration 5261 / 12250) loss: 0.911663\n",
      "(Iteration 5281 / 12250) loss: 0.855802\n",
      "(Iteration 5301 / 12250) loss: 0.929538\n",
      "(Iteration 5321 / 12250) loss: 0.828980\n",
      "(Iteration 5341 / 12250) loss: 0.891640\n",
      "(Iteration 5361 / 12250) loss: 0.814374\n",
      "(Iteration 5381 / 12250) loss: 0.894931\n",
      "(Epoch 22 / 50) train acc: 0.796000; val_acc: 0.729000\n",
      "(Iteration 5401 / 12250) loss: 0.898142\n",
      "(Iteration 5421 / 12250) loss: 0.837941\n",
      "(Iteration 5441 / 12250) loss: 0.868367\n",
      "(Iteration 5461 / 12250) loss: 0.884822\n",
      "(Iteration 5481 / 12250) loss: 0.801617\n",
      "(Iteration 5501 / 12250) loss: 0.850792\n",
      "(Iteration 5521 / 12250) loss: 0.905778\n",
      "(Iteration 5541 / 12250) loss: 0.915409\n",
      "(Iteration 5561 / 12250) loss: 0.875190\n",
      "(Iteration 5581 / 12250) loss: 0.886810\n",
      "(Iteration 5601 / 12250) loss: 0.904875\n",
      "(Iteration 5621 / 12250) loss: 0.846155\n",
      "(Epoch 23 / 50) train acc: 0.812000; val_acc: 0.730000\n",
      "(Iteration 5641 / 12250) loss: 0.919789\n",
      "(Iteration 5661 / 12250) loss: 0.806895\n",
      "(Iteration 5681 / 12250) loss: 0.852950\n",
      "(Iteration 5701 / 12250) loss: 0.918475\n",
      "(Iteration 5721 / 12250) loss: 0.859949\n",
      "(Iteration 5741 / 12250) loss: 0.780599\n",
      "(Iteration 5761 / 12250) loss: 0.928676\n",
      "(Iteration 5781 / 12250) loss: 0.794426\n",
      "(Iteration 5801 / 12250) loss: 0.873689\n",
      "(Iteration 5821 / 12250) loss: 0.830232\n",
      "(Iteration 5841 / 12250) loss: 0.847231\n",
      "(Iteration 5861 / 12250) loss: 0.851389\n",
      "(Epoch 24 / 50) train acc: 0.819000; val_acc: 0.731000\n",
      "(Iteration 5881 / 12250) loss: 0.849940\n",
      "(Iteration 5901 / 12250) loss: 0.928550\n",
      "(Iteration 5921 / 12250) loss: 0.843999\n",
      "(Iteration 5941 / 12250) loss: 0.800947\n",
      "(Iteration 5961 / 12250) loss: 0.823293\n",
      "(Iteration 5981 / 12250) loss: 0.826598\n",
      "(Iteration 6001 / 12250) loss: 0.925418\n",
      "(Iteration 6021 / 12250) loss: 0.890391\n",
      "(Iteration 6041 / 12250) loss: 0.823027\n",
      "(Iteration 6061 / 12250) loss: 0.877408\n",
      "(Iteration 6081 / 12250) loss: 0.810098\n",
      "(Iteration 6101 / 12250) loss: 0.863892\n",
      "(Iteration 6121 / 12250) loss: 0.803627\n",
      "(Epoch 25 / 50) train acc: 0.807000; val_acc: 0.735000\n",
      "(Iteration 6141 / 12250) loss: 0.808317\n",
      "(Iteration 6161 / 12250) loss: 0.787716\n",
      "(Iteration 6181 / 12250) loss: 0.811154\n",
      "(Iteration 6201 / 12250) loss: 0.797165\n",
      "(Iteration 6221 / 12250) loss: 0.853094\n",
      "(Iteration 6241 / 12250) loss: 0.912181\n",
      "(Iteration 6261 / 12250) loss: 0.808736\n",
      "(Iteration 6281 / 12250) loss: 0.952850\n",
      "(Iteration 6301 / 12250) loss: 0.918361\n",
      "(Iteration 6321 / 12250) loss: 0.871535\n",
      "(Iteration 6341 / 12250) loss: 0.742774\n",
      "(Iteration 6361 / 12250) loss: 0.750534\n",
      "(Epoch 26 / 50) train acc: 0.804000; val_acc: 0.737000\n",
      "(Iteration 6381 / 12250) loss: 0.773443\n",
      "(Iteration 6401 / 12250) loss: 0.849600\n",
      "(Iteration 6421 / 12250) loss: 0.786986\n",
      "(Iteration 6441 / 12250) loss: 0.833210\n",
      "(Iteration 6461 / 12250) loss: 0.825839\n",
      "(Iteration 6481 / 12250) loss: 0.803522\n",
      "(Iteration 6501 / 12250) loss: 0.827240\n",
      "(Iteration 6521 / 12250) loss: 0.817486\n",
      "(Iteration 6541 / 12250) loss: 0.805318\n",
      "(Iteration 6561 / 12250) loss: 0.748424\n",
      "(Iteration 6581 / 12250) loss: 0.783431\n",
      "(Iteration 6601 / 12250) loss: 0.818869\n",
      "(Epoch 27 / 50) train acc: 0.830000; val_acc: 0.732000\n",
      "(Iteration 6621 / 12250) loss: 0.759439\n",
      "(Iteration 6641 / 12250) loss: 0.868971\n",
      "(Iteration 6661 / 12250) loss: 0.685241\n",
      "(Iteration 6681 / 12250) loss: 0.858680\n",
      "(Iteration 6701 / 12250) loss: 0.828475\n",
      "(Iteration 6721 / 12250) loss: 0.743755\n",
      "(Iteration 6741 / 12250) loss: 0.817299\n",
      "(Iteration 6761 / 12250) loss: 0.831013\n",
      "(Iteration 6781 / 12250) loss: 0.767047\n",
      "(Iteration 6801 / 12250) loss: 0.759170\n",
      "(Iteration 6821 / 12250) loss: 0.771589\n",
      "(Iteration 6841 / 12250) loss: 0.845790\n",
      "(Epoch 28 / 50) train acc: 0.818000; val_acc: 0.737000\n",
      "(Iteration 6861 / 12250) loss: 0.754463\n",
      "(Iteration 6881 / 12250) loss: 0.801922\n",
      "(Iteration 6901 / 12250) loss: 0.763899\n",
      "(Iteration 6921 / 12250) loss: 0.787410\n",
      "(Iteration 6941 / 12250) loss: 0.861234\n",
      "(Iteration 6961 / 12250) loss: 0.770908\n",
      "(Iteration 6981 / 12250) loss: 0.734510\n",
      "(Iteration 7001 / 12250) loss: 0.833518\n",
      "(Iteration 7021 / 12250) loss: 0.768492\n",
      "(Iteration 7041 / 12250) loss: 0.774315\n",
      "(Iteration 7061 / 12250) loss: 0.777205\n",
      "(Iteration 7081 / 12250) loss: 0.822452\n",
      "(Iteration 7101 / 12250) loss: 0.852236\n",
      "(Epoch 29 / 50) train acc: 0.843000; val_acc: 0.745000\n",
      "(Iteration 7121 / 12250) loss: 0.693200\n",
      "(Iteration 7141 / 12250) loss: 0.769810\n",
      "(Iteration 7161 / 12250) loss: 0.817798\n",
      "(Iteration 7181 / 12250) loss: 0.833844\n",
      "(Iteration 7201 / 12250) loss: 0.783416\n",
      "(Iteration 7221 / 12250) loss: 0.796168\n",
      "(Iteration 7241 / 12250) loss: 0.719492\n",
      "(Iteration 7261 / 12250) loss: 0.764874\n",
      "(Iteration 7281 / 12250) loss: 0.726239\n",
      "(Iteration 7301 / 12250) loss: 0.792052\n",
      "(Iteration 7321 / 12250) loss: 0.795972\n",
      "(Iteration 7341 / 12250) loss: 0.794680\n",
      "(Epoch 30 / 50) train acc: 0.817000; val_acc: 0.735000\n",
      "(Iteration 7361 / 12250) loss: 0.719586\n",
      "(Iteration 7381 / 12250) loss: 0.701592\n",
      "(Iteration 7401 / 12250) loss: 0.799357\n",
      "(Iteration 7421 / 12250) loss: 0.815379\n",
      "(Iteration 7441 / 12250) loss: 0.706328\n",
      "(Iteration 7461 / 12250) loss: 0.797419\n",
      "(Iteration 7481 / 12250) loss: 0.758657\n",
      "(Iteration 7501 / 12250) loss: 0.842998\n",
      "(Iteration 7521 / 12250) loss: 0.770876\n",
      "(Iteration 7541 / 12250) loss: 0.793206\n",
      "(Iteration 7561 / 12250) loss: 0.786746\n",
      "(Iteration 7581 / 12250) loss: 0.733347\n",
      "(Epoch 31 / 50) train acc: 0.816000; val_acc: 0.734000\n",
      "(Iteration 7601 / 12250) loss: 0.731047\n",
      "(Iteration 7621 / 12250) loss: 0.828777\n",
      "(Iteration 7641 / 12250) loss: 0.740928\n",
      "(Iteration 7661 / 12250) loss: 0.786461\n",
      "(Iteration 7681 / 12250) loss: 0.739048\n",
      "(Iteration 7701 / 12250) loss: 0.721359\n",
      "(Iteration 7721 / 12250) loss: 0.798117\n",
      "(Iteration 7741 / 12250) loss: 0.769825\n",
      "(Iteration 7761 / 12250) loss: 0.756082\n",
      "(Iteration 7781 / 12250) loss: 0.793614\n",
      "(Iteration 7801 / 12250) loss: 0.758531\n",
      "(Iteration 7821 / 12250) loss: 0.775539\n",
      "(Epoch 32 / 50) train acc: 0.834000; val_acc: 0.734000\n",
      "(Iteration 7841 / 12250) loss: 0.771656\n",
      "(Iteration 7861 / 12250) loss: 0.705351\n",
      "(Iteration 7881 / 12250) loss: 0.722740\n",
      "(Iteration 7901 / 12250) loss: 0.712150\n",
      "(Iteration 7921 / 12250) loss: 0.749230\n",
      "(Iteration 7941 / 12250) loss: 0.731920\n",
      "(Iteration 7961 / 12250) loss: 0.684851\n",
      "(Iteration 7981 / 12250) loss: 0.767991\n",
      "(Iteration 8001 / 12250) loss: 0.719768\n",
      "(Iteration 8021 / 12250) loss: 0.751489\n",
      "(Iteration 8041 / 12250) loss: 0.773429\n",
      "(Iteration 8061 / 12250) loss: 0.710531\n",
      "(Iteration 8081 / 12250) loss: 0.726621\n",
      "(Epoch 33 / 50) train acc: 0.819000; val_acc: 0.741000\n",
      "(Iteration 8101 / 12250) loss: 0.722337\n",
      "(Iteration 8121 / 12250) loss: 0.743281\n",
      "(Iteration 8141 / 12250) loss: 0.695611\n",
      "(Iteration 8161 / 12250) loss: 0.717730\n",
      "(Iteration 8181 / 12250) loss: 0.753100\n",
      "(Iteration 8201 / 12250) loss: 0.702578\n",
      "(Iteration 8221 / 12250) loss: 0.734479\n",
      "(Iteration 8241 / 12250) loss: 0.721592\n",
      "(Iteration 8261 / 12250) loss: 0.706090\n",
      "(Iteration 8281 / 12250) loss: 0.754726\n",
      "(Iteration 8301 / 12250) loss: 0.762694\n",
      "(Iteration 8321 / 12250) loss: 0.751095\n",
      "(Epoch 34 / 50) train acc: 0.840000; val_acc: 0.739000\n",
      "(Iteration 8341 / 12250) loss: 0.748062\n",
      "(Iteration 8361 / 12250) loss: 0.704704\n",
      "(Iteration 8381 / 12250) loss: 0.671053\n",
      "(Iteration 8401 / 12250) loss: 0.740095\n",
      "(Iteration 8421 / 12250) loss: 0.782239\n",
      "(Iteration 8441 / 12250) loss: 0.759085\n",
      "(Iteration 8461 / 12250) loss: 0.695853\n",
      "(Iteration 8481 / 12250) loss: 0.676538\n",
      "(Iteration 8501 / 12250) loss: 0.731895\n",
      "(Iteration 8521 / 12250) loss: 0.719668\n",
      "(Iteration 8541 / 12250) loss: 0.744965\n",
      "(Iteration 8561 / 12250) loss: 0.778782\n",
      "(Epoch 35 / 50) train acc: 0.842000; val_acc: 0.734000\n",
      "(Iteration 8581 / 12250) loss: 0.741385\n",
      "(Iteration 8601 / 12250) loss: 0.747973\n",
      "(Iteration 8621 / 12250) loss: 0.788359\n",
      "(Iteration 8641 / 12250) loss: 0.689557\n",
      "(Iteration 8661 / 12250) loss: 0.837754\n",
      "(Iteration 8681 / 12250) loss: 0.765596\n",
      "(Iteration 8701 / 12250) loss: 0.724996\n",
      "(Iteration 8721 / 12250) loss: 0.745987\n",
      "(Iteration 8741 / 12250) loss: 0.716199\n",
      "(Iteration 8761 / 12250) loss: 0.716507\n",
      "(Iteration 8781 / 12250) loss: 0.700734\n",
      "(Iteration 8801 / 12250) loss: 0.788231\n",
      "(Epoch 36 / 50) train acc: 0.815000; val_acc: 0.732000\n",
      "(Iteration 8821 / 12250) loss: 0.752542\n",
      "(Iteration 8841 / 12250) loss: 0.728649\n",
      "(Iteration 8861 / 12250) loss: 0.783530\n",
      "(Iteration 8881 / 12250) loss: 0.736478\n",
      "(Iteration 8901 / 12250) loss: 0.714134\n",
      "(Iteration 8921 / 12250) loss: 0.708052\n",
      "(Iteration 8941 / 12250) loss: 0.704579\n",
      "(Iteration 8961 / 12250) loss: 0.704714\n",
      "(Iteration 8981 / 12250) loss: 0.761583\n",
      "(Iteration 9001 / 12250) loss: 0.760129\n",
      "(Iteration 9021 / 12250) loss: 0.661619\n",
      "(Iteration 9041 / 12250) loss: 0.760591\n",
      "(Iteration 9061 / 12250) loss: 0.700291\n",
      "(Epoch 37 / 50) train acc: 0.835000; val_acc: 0.748000\n",
      "(Iteration 9081 / 12250) loss: 0.756504\n",
      "(Iteration 9101 / 12250) loss: 0.724591\n",
      "(Iteration 9121 / 12250) loss: 0.661720\n",
      "(Iteration 9141 / 12250) loss: 0.726695\n",
      "(Iteration 9161 / 12250) loss: 0.735266\n",
      "(Iteration 9181 / 12250) loss: 0.707928\n",
      "(Iteration 9201 / 12250) loss: 0.681696\n",
      "(Iteration 9221 / 12250) loss: 0.673338\n",
      "(Iteration 9241 / 12250) loss: 0.714571\n",
      "(Iteration 9261 / 12250) loss: 0.683615\n",
      "(Iteration 9281 / 12250) loss: 0.674070\n",
      "(Iteration 9301 / 12250) loss: 0.696350\n",
      "(Epoch 38 / 50) train acc: 0.836000; val_acc: 0.741000\n",
      "(Iteration 9321 / 12250) loss: 0.777699\n",
      "(Iteration 9341 / 12250) loss: 0.665267\n",
      "(Iteration 9361 / 12250) loss: 0.693320\n",
      "(Iteration 9381 / 12250) loss: 0.751242\n",
      "(Iteration 9401 / 12250) loss: 0.725180\n",
      "(Iteration 9421 / 12250) loss: 0.737486\n",
      "(Iteration 9441 / 12250) loss: 0.759381\n",
      "(Iteration 9461 / 12250) loss: 0.723855\n",
      "(Iteration 9481 / 12250) loss: 0.645553\n",
      "(Iteration 9501 / 12250) loss: 0.681777\n",
      "(Iteration 9521 / 12250) loss: 0.707429\n",
      "(Iteration 9541 / 12250) loss: 0.767108\n",
      "(Epoch 39 / 50) train acc: 0.822000; val_acc: 0.739000\n",
      "(Iteration 9561 / 12250) loss: 0.704569\n",
      "(Iteration 9581 / 12250) loss: 0.706662\n",
      "(Iteration 9601 / 12250) loss: 0.806012\n",
      "(Iteration 9621 / 12250) loss: 0.717482\n",
      "(Iteration 9641 / 12250) loss: 0.748135\n",
      "(Iteration 9661 / 12250) loss: 0.701575\n",
      "(Iteration 9681 / 12250) loss: 0.832442\n",
      "(Iteration 9701 / 12250) loss: 0.641386\n",
      "(Iteration 9721 / 12250) loss: 0.718820\n",
      "(Iteration 9741 / 12250) loss: 0.745317\n",
      "(Iteration 9761 / 12250) loss: 0.678104\n",
      "(Iteration 9781 / 12250) loss: 0.699275\n",
      "(Epoch 40 / 50) train acc: 0.830000; val_acc: 0.739000\n",
      "(Iteration 9801 / 12250) loss: 0.755866\n",
      "(Iteration 9821 / 12250) loss: 0.698752\n",
      "(Iteration 9841 / 12250) loss: 0.657489\n",
      "(Iteration 9861 / 12250) loss: 0.727018\n",
      "(Iteration 9881 / 12250) loss: 0.699882\n",
      "(Iteration 9901 / 12250) loss: 0.713554\n",
      "(Iteration 9921 / 12250) loss: 0.722089\n",
      "(Iteration 9941 / 12250) loss: 0.741566\n",
      "(Iteration 9961 / 12250) loss: 0.668162\n",
      "(Iteration 9981 / 12250) loss: 0.698753\n",
      "(Iteration 10001 / 12250) loss: 0.805622\n",
      "(Iteration 10021 / 12250) loss: 0.680732\n",
      "(Iteration 10041 / 12250) loss: 0.719240\n",
      "(Epoch 41 / 50) train acc: 0.815000; val_acc: 0.746000\n",
      "(Iteration 10061 / 12250) loss: 0.732926\n",
      "(Iteration 10081 / 12250) loss: 0.661325\n",
      "(Iteration 10101 / 12250) loss: 0.796846\n",
      "(Iteration 10121 / 12250) loss: 0.726285\n",
      "(Iteration 10141 / 12250) loss: 0.663317\n",
      "(Iteration 10161 / 12250) loss: 0.642034\n",
      "(Iteration 10181 / 12250) loss: 0.735314\n",
      "(Iteration 10201 / 12250) loss: 0.652003\n",
      "(Iteration 10221 / 12250) loss: 0.746716\n",
      "(Iteration 10241 / 12250) loss: 0.694011\n",
      "(Iteration 10261 / 12250) loss: 0.806019\n",
      "(Iteration 10281 / 12250) loss: 0.740874\n",
      "(Epoch 42 / 50) train acc: 0.831000; val_acc: 0.740000\n",
      "(Iteration 10301 / 12250) loss: 0.739618\n",
      "(Iteration 10321 / 12250) loss: 0.795257\n",
      "(Iteration 10341 / 12250) loss: 0.684486\n",
      "(Iteration 10361 / 12250) loss: 0.662808\n",
      "(Iteration 10381 / 12250) loss: 0.716948\n",
      "(Iteration 10401 / 12250) loss: 0.646734\n",
      "(Iteration 10421 / 12250) loss: 0.711081\n",
      "(Iteration 10441 / 12250) loss: 0.731267\n",
      "(Iteration 10461 / 12250) loss: 0.678246\n",
      "(Iteration 10481 / 12250) loss: 0.640915\n",
      "(Iteration 10501 / 12250) loss: 0.688303\n",
      "(Iteration 10521 / 12250) loss: 0.825328\n",
      "(Epoch 43 / 50) train acc: 0.852000; val_acc: 0.746000\n",
      "(Iteration 10541 / 12250) loss: 0.701091\n",
      "(Iteration 10561 / 12250) loss: 0.728735\n",
      "(Iteration 10581 / 12250) loss: 0.732870\n",
      "(Iteration 10601 / 12250) loss: 0.687267\n",
      "(Iteration 10621 / 12250) loss: 0.726759\n",
      "(Iteration 10641 / 12250) loss: 0.728377\n",
      "(Iteration 10661 / 12250) loss: 0.665472\n",
      "(Iteration 10681 / 12250) loss: 0.731656\n",
      "(Iteration 10701 / 12250) loss: 0.721875\n",
      "(Iteration 10721 / 12250) loss: 0.690227\n",
      "(Iteration 10741 / 12250) loss: 0.648886\n",
      "(Iteration 10761 / 12250) loss: 0.679030\n",
      "(Epoch 44 / 50) train acc: 0.839000; val_acc: 0.743000\n",
      "(Iteration 10781 / 12250) loss: 0.726421\n",
      "(Iteration 10801 / 12250) loss: 0.688263\n",
      "(Iteration 10821 / 12250) loss: 0.701769\n",
      "(Iteration 10841 / 12250) loss: 0.718296\n",
      "(Iteration 10861 / 12250) loss: 0.641076\n",
      "(Iteration 10881 / 12250) loss: 0.728020\n",
      "(Iteration 10901 / 12250) loss: 0.651172\n",
      "(Iteration 10921 / 12250) loss: 0.655799\n",
      "(Iteration 10941 / 12250) loss: 0.663559\n",
      "(Iteration 10961 / 12250) loss: 0.645057\n",
      "(Iteration 10981 / 12250) loss: 0.621204\n",
      "(Iteration 11001 / 12250) loss: 0.690632\n",
      "(Iteration 11021 / 12250) loss: 0.664767\n",
      "(Epoch 45 / 50) train acc: 0.841000; val_acc: 0.739000\n",
      "(Iteration 11041 / 12250) loss: 0.609221\n",
      "(Iteration 11061 / 12250) loss: 0.645338\n",
      "(Iteration 11081 / 12250) loss: 0.664988\n",
      "(Iteration 11101 / 12250) loss: 0.646031\n",
      "(Iteration 11121 / 12250) loss: 0.607464\n",
      "(Iteration 11141 / 12250) loss: 0.669624\n",
      "(Iteration 11161 / 12250) loss: 0.714662\n",
      "(Iteration 11181 / 12250) loss: 0.624077\n",
      "(Iteration 11201 / 12250) loss: 0.651958\n",
      "(Iteration 11221 / 12250) loss: 0.655712\n",
      "(Iteration 11241 / 12250) loss: 0.663538\n",
      "(Iteration 11261 / 12250) loss: 0.704474\n",
      "(Epoch 46 / 50) train acc: 0.841000; val_acc: 0.739000\n",
      "(Iteration 11281 / 12250) loss: 0.638332\n",
      "(Iteration 11301 / 12250) loss: 0.695067\n",
      "(Iteration 11321 / 12250) loss: 0.725147\n",
      "(Iteration 11341 / 12250) loss: 0.712204\n",
      "(Iteration 11361 / 12250) loss: 0.732708\n",
      "(Iteration 11381 / 12250) loss: 0.710090\n",
      "(Iteration 11401 / 12250) loss: 0.656078\n",
      "(Iteration 11421 / 12250) loss: 0.699234\n",
      "(Iteration 11441 / 12250) loss: 0.729042\n",
      "(Iteration 11461 / 12250) loss: 0.669744\n",
      "(Iteration 11481 / 12250) loss: 0.635045\n",
      "(Iteration 11501 / 12250) loss: 0.711877\n",
      "(Epoch 47 / 50) train acc: 0.847000; val_acc: 0.737000\n",
      "(Iteration 11521 / 12250) loss: 0.689500\n",
      "(Iteration 11541 / 12250) loss: 0.613307\n",
      "(Iteration 11561 / 12250) loss: 0.702883\n",
      "(Iteration 11581 / 12250) loss: 0.692538\n",
      "(Iteration 11601 / 12250) loss: 0.626911\n",
      "(Iteration 11621 / 12250) loss: 0.667309\n",
      "(Iteration 11641 / 12250) loss: 0.604631\n",
      "(Iteration 11661 / 12250) loss: 0.692497\n",
      "(Iteration 11681 / 12250) loss: 0.606388\n",
      "(Iteration 11701 / 12250) loss: 0.665410\n",
      "(Iteration 11721 / 12250) loss: 0.655394\n",
      "(Iteration 11741 / 12250) loss: 0.731416\n",
      "(Epoch 48 / 50) train acc: 0.835000; val_acc: 0.740000\n",
      "(Iteration 11761 / 12250) loss: 0.769863\n",
      "(Iteration 11781 / 12250) loss: 0.701379\n",
      "(Iteration 11801 / 12250) loss: 0.692934\n",
      "(Iteration 11821 / 12250) loss: 0.599778\n",
      "(Iteration 11841 / 12250) loss: 0.654959\n",
      "(Iteration 11861 / 12250) loss: 0.677808\n",
      "(Iteration 11881 / 12250) loss: 0.632480\n",
      "(Iteration 11901 / 12250) loss: 0.715283\n",
      "(Iteration 11921 / 12250) loss: 0.741245\n",
      "(Iteration 11941 / 12250) loss: 0.713327\n",
      "(Iteration 11961 / 12250) loss: 0.701444\n",
      "(Iteration 11981 / 12250) loss: 0.682918\n",
      "(Iteration 12001 / 12250) loss: 0.684847\n",
      "(Epoch 49 / 50) train acc: 0.842000; val_acc: 0.741000\n",
      "(Iteration 12021 / 12250) loss: 0.612014\n",
      "(Iteration 12041 / 12250) loss: 0.689724\n",
      "(Iteration 12061 / 12250) loss: 0.646082\n",
      "(Iteration 12081 / 12250) loss: 0.672683\n",
      "(Iteration 12101 / 12250) loss: 0.691515\n",
      "(Iteration 12121 / 12250) loss: 0.757648\n",
      "(Iteration 12141 / 12250) loss: 0.677817\n",
      "(Iteration 12161 / 12250) loss: 0.723454\n",
      "(Iteration 12181 / 12250) loss: 0.720306\n",
      "(Iteration 12201 / 12250) loss: 0.655819\n",
      "(Iteration 12221 / 12250) loss: 0.666936\n",
      "(Iteration 12241 / 12250) loss: 0.671988\n",
      "(Epoch 50 / 50) train acc: 0.838000; val_acc: 0.744000\n"
     ]
    }
   ],
   "source": [
    "# set up learning parameters\n",
    "learning_rate = 10**(-5) # initial learning rate\n",
    "weight_scale = 10**(-3) # for weight initialization\n",
    "\n",
    "# construct learning model, here is full connected neural network\n",
    "# Here the network structure is:\n",
    "# {conv-spatialbatchnorm-relu-max pool}x2-{affine-batchnorm-relu-dropout}x1-affine-softmax\n",
    "# num_filters parameter specifies the filter number in each convolution layer, '0' represents using max pool\n",
    "# hidden_dim parameter specifies the dimension of each full connected hidden layer\n",
    "model1 = ConvNet(input_dim=[3,32,32],num_filters=[32,0,64,0], filter_size=3,\n",
    "                hidden_dim=[256],num_classes=10,filter_stride=1, pad=None, \n",
    "                pool_size=2, pool_stride=2, weight_scale=weight_scale, reg=0.01, \n",
    "                dtype=np.float32, seed=None, use_batchnorm=True, dropout=0.5)\n",
    "\n",
    "# construct a solver for the model above\n",
    "solver1 = Solver(model1, data,\n",
    "                num_epochs=50, batch_size=200,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                },\n",
    "                lr_decay=0.95,\n",
    "                verbose=True, print_every=20)\n",
    "\n",
    "# start training\n",
    "solver1.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Validation set accuracy:  0.753\n",
      "Model 1 Test set accuracy:  0.742\n"
     ]
    }
   ],
   "source": [
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "y_test_pred = np.argmax(model1.loss(X_test), axis=1)\n",
    "y_val_pred = np.argmax(model1.loss(X_val), axis=1)\n",
    "print 'Model 1 Validation set accuracy: ', (y_val_pred == y_val).mean()\n",
    "print 'Model 1 Test set accuracy: ', (y_test_pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 12250) loss: 2.308429\n",
      "(Epoch 0 / 50) train acc: 0.121000; val_acc: 0.110000\n",
      "(Iteration 101 / 12250) loss: 2.211185\n",
      "(Iteration 201 / 12250) loss: 2.103051\n",
      "(Epoch 1 / 50) train acc: 0.467000; val_acc: 0.513000\n",
      "(Iteration 301 / 12250) loss: 2.001738\n",
      "(Iteration 401 / 12250) loss: 1.886993\n",
      "(Epoch 2 / 50) train acc: 0.577000; val_acc: 0.579000\n",
      "(Iteration 501 / 12250) loss: 1.830736\n",
      "(Iteration 601 / 12250) loss: 1.793744\n",
      "(Iteration 701 / 12250) loss: 1.714917\n",
      "(Epoch 3 / 50) train acc: 0.646000; val_acc: 0.617000\n",
      "(Iteration 801 / 12250) loss: 1.578736\n",
      "(Iteration 901 / 12250) loss: 1.548976\n",
      "(Epoch 4 / 50) train acc: 0.666000; val_acc: 0.653000\n",
      "(Iteration 1001 / 12250) loss: 1.500658\n",
      "(Iteration 1101 / 12250) loss: 1.480080\n",
      "(Iteration 1201 / 12250) loss: 1.403755\n",
      "(Epoch 5 / 50) train acc: 0.705000; val_acc: 0.675000\n",
      "(Iteration 1301 / 12250) loss: 1.388039\n",
      "(Iteration 1401 / 12250) loss: 1.335991\n",
      "(Epoch 6 / 50) train acc: 0.729000; val_acc: 0.692000\n",
      "(Iteration 1501 / 12250) loss: 1.317105\n",
      "(Iteration 1601 / 12250) loss: 1.288264\n",
      "(Iteration 1701 / 12250) loss: 1.260870\n",
      "(Epoch 7 / 50) train acc: 0.743000; val_acc: 0.701000\n",
      "(Iteration 1801 / 12250) loss: 1.196911\n",
      "(Iteration 1901 / 12250) loss: 1.118753\n",
      "(Epoch 8 / 50) train acc: 0.736000; val_acc: 0.714000\n",
      "(Iteration 2001 / 12250) loss: 1.083318\n",
      "(Iteration 2101 / 12250) loss: 1.110952\n",
      "(Iteration 2201 / 12250) loss: 1.091794\n",
      "(Epoch 9 / 50) train acc: 0.789000; val_acc: 0.719000\n",
      "(Iteration 2301 / 12250) loss: 1.035170\n",
      "(Iteration 2401 / 12250) loss: 0.986027\n",
      "(Epoch 10 / 50) train acc: 0.771000; val_acc: 0.715000\n",
      "(Iteration 2501 / 12250) loss: 1.019351\n",
      "(Iteration 2601 / 12250) loss: 0.970155\n",
      "(Epoch 11 / 50) train acc: 0.793000; val_acc: 0.725000\n",
      "(Iteration 2701 / 12250) loss: 0.970371\n",
      "(Iteration 2801 / 12250) loss: 0.875044\n",
      "(Iteration 2901 / 12250) loss: 0.942691\n",
      "(Epoch 12 / 50) train acc: 0.788000; val_acc: 0.727000\n",
      "(Iteration 3001 / 12250) loss: 0.914650\n",
      "(Iteration 3101 / 12250) loss: 0.847833\n",
      "(Epoch 13 / 50) train acc: 0.819000; val_acc: 0.726000\n",
      "(Iteration 3201 / 12250) loss: 0.811154\n",
      "(Iteration 3301 / 12250) loss: 0.809036\n",
      "(Iteration 3401 / 12250) loss: 0.845676\n",
      "(Epoch 14 / 50) train acc: 0.839000; val_acc: 0.731000\n",
      "(Iteration 3501 / 12250) loss: 0.806975\n",
      "(Iteration 3601 / 12250) loss: 0.813258\n",
      "(Epoch 15 / 50) train acc: 0.823000; val_acc: 0.733000\n",
      "(Iteration 3701 / 12250) loss: 0.737365\n",
      "(Iteration 3801 / 12250) loss: 0.785809\n",
      "(Iteration 3901 / 12250) loss: 0.798560\n",
      "(Epoch 16 / 50) train acc: 0.820000; val_acc: 0.754000\n",
      "(Iteration 4001 / 12250) loss: 0.722056\n",
      "(Iteration 4101 / 12250) loss: 0.736486\n",
      "(Epoch 17 / 50) train acc: 0.857000; val_acc: 0.748000\n",
      "(Iteration 4201 / 12250) loss: 0.712090\n",
      "(Iteration 4301 / 12250) loss: 0.681630\n",
      "(Iteration 4401 / 12250) loss: 0.619256\n",
      "(Epoch 18 / 50) train acc: 0.859000; val_acc: 0.740000\n",
      "(Iteration 4501 / 12250) loss: 0.731126\n",
      "(Iteration 4601 / 12250) loss: 0.654150\n",
      "(Epoch 19 / 50) train acc: 0.838000; val_acc: 0.738000\n",
      "(Iteration 4701 / 12250) loss: 0.625123\n",
      "(Iteration 4801 / 12250) loss: 0.644998\n",
      "(Epoch 20 / 50) train acc: 0.855000; val_acc: 0.738000\n",
      "(Iteration 4901 / 12250) loss: 0.588931\n",
      "(Iteration 5001 / 12250) loss: 0.673082\n",
      "(Iteration 5101 / 12250) loss: 0.588555\n",
      "(Epoch 21 / 50) train acc: 0.881000; val_acc: 0.732000\n",
      "(Iteration 5201 / 12250) loss: 0.630318\n",
      "(Iteration 5301 / 12250) loss: 0.643201\n",
      "(Epoch 22 / 50) train acc: 0.857000; val_acc: 0.742000\n",
      "(Iteration 5401 / 12250) loss: 0.641593\n",
      "(Iteration 5501 / 12250) loss: 0.603773\n",
      "(Iteration 5601 / 12250) loss: 0.581317\n",
      "(Epoch 23 / 50) train acc: 0.901000; val_acc: 0.748000\n",
      "(Iteration 5701 / 12250) loss: 0.551322\n",
      "(Iteration 5801 / 12250) loss: 0.567485\n",
      "(Epoch 24 / 50) train acc: 0.883000; val_acc: 0.739000\n",
      "(Iteration 5901 / 12250) loss: 0.560445\n",
      "(Iteration 6001 / 12250) loss: 0.562510\n",
      "(Iteration 6101 / 12250) loss: 0.524964\n",
      "(Epoch 25 / 50) train acc: 0.898000; val_acc: 0.747000\n",
      "(Iteration 6201 / 12250) loss: 0.488181\n",
      "(Iteration 6301 / 12250) loss: 0.472543\n",
      "(Epoch 26 / 50) train acc: 0.891000; val_acc: 0.744000\n",
      "(Iteration 6401 / 12250) loss: 0.545036\n",
      "(Iteration 6501 / 12250) loss: 0.502790\n",
      "(Iteration 6601 / 12250) loss: 0.473313\n",
      "(Epoch 27 / 50) train acc: 0.902000; val_acc: 0.742000\n",
      "(Iteration 6701 / 12250) loss: 0.532999\n",
      "(Iteration 6801 / 12250) loss: 0.521165\n",
      "(Epoch 28 / 50) train acc: 0.905000; val_acc: 0.747000\n",
      "(Iteration 6901 / 12250) loss: 0.496572\n",
      "(Iteration 7001 / 12250) loss: 0.499156\n",
      "(Iteration 7101 / 12250) loss: 0.537105\n",
      "(Epoch 29 / 50) train acc: 0.907000; val_acc: 0.745000\n",
      "(Iteration 7201 / 12250) loss: 0.537577\n",
      "(Iteration 7301 / 12250) loss: 0.441882\n",
      "(Epoch 30 / 50) train acc: 0.909000; val_acc: 0.747000\n",
      "(Iteration 7401 / 12250) loss: 0.502314\n",
      "(Iteration 7501 / 12250) loss: 0.428694\n",
      "(Epoch 31 / 50) train acc: 0.902000; val_acc: 0.745000\n",
      "(Iteration 7601 / 12250) loss: 0.438089\n",
      "(Iteration 7701 / 12250) loss: 0.430515\n",
      "(Iteration 7801 / 12250) loss: 0.455092\n",
      "(Epoch 32 / 50) train acc: 0.920000; val_acc: 0.747000\n",
      "(Iteration 7901 / 12250) loss: 0.435403\n",
      "(Iteration 8001 / 12250) loss: 0.437518\n",
      "(Epoch 33 / 50) train acc: 0.940000; val_acc: 0.759000\n",
      "(Iteration 8101 / 12250) loss: 0.484068\n",
      "(Iteration 8201 / 12250) loss: 0.473475\n",
      "(Iteration 8301 / 12250) loss: 0.474715\n",
      "(Epoch 34 / 50) train acc: 0.916000; val_acc: 0.754000\n",
      "(Iteration 8401 / 12250) loss: 0.464560\n",
      "(Iteration 8501 / 12250) loss: 0.444775\n",
      "(Epoch 35 / 50) train acc: 0.938000; val_acc: 0.761000\n",
      "(Iteration 8601 / 12250) loss: 0.401257\n",
      "(Iteration 8701 / 12250) loss: 0.436094\n",
      "(Iteration 8801 / 12250) loss: 0.418880\n",
      "(Epoch 36 / 50) train acc: 0.928000; val_acc: 0.771000\n",
      "(Iteration 8901 / 12250) loss: 0.411491\n",
      "(Iteration 9001 / 12250) loss: 0.435181\n",
      "(Epoch 37 / 50) train acc: 0.945000; val_acc: 0.762000\n",
      "(Iteration 9101 / 12250) loss: 0.406650\n",
      "(Iteration 9201 / 12250) loss: 0.353400\n",
      "(Iteration 9301 / 12250) loss: 0.444286\n",
      "(Epoch 38 / 50) train acc: 0.936000; val_acc: 0.769000\n",
      "(Iteration 9401 / 12250) loss: 0.398132\n",
      "(Iteration 9501 / 12250) loss: 0.447409\n",
      "(Epoch 39 / 50) train acc: 0.931000; val_acc: 0.759000\n",
      "(Iteration 9601 / 12250) loss: 0.376436\n",
      "(Iteration 9701 / 12250) loss: 0.401616\n",
      "(Epoch 40 / 50) train acc: 0.935000; val_acc: 0.766000\n",
      "(Iteration 9801 / 12250) loss: 0.439566\n",
      "(Iteration 9901 / 12250) loss: 0.444639\n",
      "(Iteration 10001 / 12250) loss: 0.370060\n",
      "(Epoch 41 / 50) train acc: 0.946000; val_acc: 0.761000\n",
      "(Iteration 10101 / 12250) loss: 0.396769\n",
      "(Iteration 10201 / 12250) loss: 0.393513\n",
      "(Epoch 42 / 50) train acc: 0.938000; val_acc: 0.765000\n",
      "(Iteration 10301 / 12250) loss: 0.391317\n",
      "(Iteration 10401 / 12250) loss: 0.442634\n",
      "(Iteration 10501 / 12250) loss: 0.357237\n",
      "(Epoch 43 / 50) train acc: 0.949000; val_acc: 0.757000\n",
      "(Iteration 10601 / 12250) loss: 0.381950\n",
      "(Iteration 10701 / 12250) loss: 0.431416\n",
      "(Epoch 44 / 50) train acc: 0.956000; val_acc: 0.758000\n",
      "(Iteration 10801 / 12250) loss: 0.359107\n",
      "(Iteration 10901 / 12250) loss: 0.392832\n",
      "(Iteration 11001 / 12250) loss: 0.356377\n",
      "(Epoch 45 / 50) train acc: 0.957000; val_acc: 0.763000\n",
      "(Iteration 11101 / 12250) loss: 0.368086\n",
      "(Iteration 11201 / 12250) loss: 0.342949\n",
      "(Epoch 46 / 50) train acc: 0.942000; val_acc: 0.764000\n",
      "(Iteration 11301 / 12250) loss: 0.369571\n",
      "(Iteration 11401 / 12250) loss: 0.416369\n",
      "(Iteration 11501 / 12250) loss: 0.352180\n",
      "(Epoch 47 / 50) train acc: 0.955000; val_acc: 0.767000\n",
      "(Iteration 11601 / 12250) loss: 0.349992\n",
      "(Iteration 11701 / 12250) loss: 0.348303\n",
      "(Epoch 48 / 50) train acc: 0.953000; val_acc: 0.760000\n",
      "(Iteration 11801 / 12250) loss: 0.330150\n",
      "(Iteration 11901 / 12250) loss: 0.344584\n",
      "(Iteration 12001 / 12250) loss: 0.354140\n",
      "(Epoch 49 / 50) train acc: 0.944000; val_acc: 0.765000\n",
      "(Iteration 12101 / 12250) loss: 0.415270\n",
      "(Iteration 12201 / 12250) loss: 0.414210\n",
      "(Epoch 50 / 50) train acc: 0.954000; val_acc: 0.757000\n"
     ]
    }
   ],
   "source": [
    "# set up learning parameters\n",
    "learning_rate = 10**(-5) # initial learning rate\n",
    "weight_scale = 10**(-3) # for weight initialization\n",
    "\n",
    "# construct learning model, here is full connected neural network\n",
    "# Here the network structure is:\n",
    "# {conv-spatialbatchnorm-relu-max pool}x2-{affine-batchnorm-relu-dropout}x1-affine-softmax\n",
    "# num_filters parameter specifies the filter number in each convolution layer, '0' represents using max pool\n",
    "# hidden_dim parameter specifies the dimension of each full connected hidden layer\n",
    "model2 = ConvNet(input_dim=[3,32,32],num_filters=[32,32,0,64,64,0], filter_size=3,\n",
    "                hidden_dim=[256,256],num_classes=10,filter_stride=1, pad=None, \n",
    "                pool_size=2, pool_stride=2, weight_scale=weight_scale, reg=0.01, \n",
    "                dtype=np.float32, seed=None, use_batchnorm=True, dropout=0.5)\n",
    "\n",
    "# construct a solver for the model above\n",
    "solver2 = Solver(model2, data,\n",
    "                num_epochs=50, batch_size=200,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                },\n",
    "                lr_decay=0.95,\n",
    "                verbose=True, print_every=100)\n",
    "\n",
    "# start training\n",
    "solver2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 Validation set accuracy:  0.771\n",
      "Model 2 Test set accuracy:  0.753\n"
     ]
    }
   ],
   "source": [
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "y_test_pred = np.argmax(model2.loss(X_test), axis=1)\n",
    "y_val_pred = np.argmax(model2.loss(X_val), axis=1)\n",
    "print 'Model 2 Validation set accuracy: ', (y_val_pred == y_val).mean()\n",
    "print 'Model 2 Test set accuracy: ', (y_test_pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Validation set accuracy:  0.776\n",
      "Ensemble Model Test set accuracy:  0.78\n"
     ]
    }
   ],
   "source": [
    "avgscores=(model1.loss(data['X_val'])+model2.loss(data['X_val']))/2\n",
    "\n",
    "y_test_pred = np.argmax(avgscores, axis=1)\n",
    "print 'Ensemble Model Validation set accuracy: ', (y_test_pred == data['y_val']).mean()\n",
    "\n",
    "avgscores=(model1.loss(X_test)+model2.loss(X_test))/2\n",
    "\n",
    "y_test_pred = np.argmax(avgscores, axis=1)\n",
    "print 'Ensemble Model Test set accuracy: ', (y_test_pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
